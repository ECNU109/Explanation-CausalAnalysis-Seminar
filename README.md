# Explanation-CausalAnalysis-Seminar
2022Spring Seminar: 1. Explanation Technique 2. Graph Explainability 3. Causal Analysis

时间：周二 9:00 am

地点：理科大楼A1716

---

## 论文报告安排

|  时间   | 报告人  | 主题  | 论文  | 资源  |
|  ----  | ---- | ----  | ----  | ----  |
| 3.8      | 纪焘 | Intro. | [AAAI21 Tutorial] [Explaining Machine Learning Predictions: State-of-the-art, Challenges, and Opportunities](https://explainml-tutorial.github.io/aaai21) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0308-纪焘-Intro.pdf)  <br> [video](https://pan.baidu.com/s/1Oq9QCDOB37iRdCH9F1kUOA?pwd=kqde) |
| 3.15      | 胡梦琦 | GNN Exp.+计算病理 | [CVPR21] [Quantifying Explainers of Graph Neural Networks in Computational Pathology.](https://arxiv.org/pdf/2011.12646.pdf) <br> [ICML21] [Improving Molecular Graph Neural Network Explainability with Orthonormalization and Induced Sparsity.](https://arxiv.org/abs/2105.04854) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0315-胡梦琦-GNN计算病理可解释性.pptx)  <br> [video](https://pan.baidu.com/s/1c0AH_-ou5rjXNHdgcyMkxg?pwd=gg5q) <br> [summary](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/Paper%20files/胡梦琦-GNN可解释性在计算病理中的应用.doc) |
| 3.15      | 屈稳稳 | Evaluating(CV) | [FAT19] [On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection.](https://arxiv.org/pdf/1811.07901.pdf) <br> [AAAI20] [Visualizing Deep Networks by Optimizing with Integrated Gradients.](https://arxiv.org/pdf/1905.00954.pdf) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0315-屈稳稳-Evaluating%20CV.pptx)  <br> [video](https://pan.baidu.com/s/1c0AH_-ou5rjXNHdgcyMkxg?pwd=gg5q) <br> [summary]() |
| 3.22      | 李放     | GNN Exp.2 | [KDD20] [Xgnn: Towards model-level explanations of graph neural networks.](https://dl.acm.org/doi/pdf/10.1145/3394486.3403085) <br> [KDD21] [When Comparing to Ground Truth is Wrong: On Evaluating GNN Explanation Methods.](https://dl.acm.org/doi/10.1145/3447548.3467283) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0322-李放-GNN%20Exp.2.pptx)  <br> [video](https://pan.baidu.com/s/1YxRK7dM8IchfXSMCqoi9JA?pwd=6shh) <br> [summary](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/Paper%20files/李放%2BGNN可解释性.dotx) |
| 3.22      | 李文锋 | GNN Exp.+社交网络 | [ACL20] [GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media.](https://arxiv.org/pdf/2004.11648.pdf) <br> [EMNLP20] [HENIN: Learning Heterogeneous Neural Interaction Networks for Explainable Cyberbullying Detection on Social Media.](https://www.aclweb.org/anthology/2020.emnlp-main.200/) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0322-李文锋-GNN%20Exp.%2B社交网络.pptx)  <br> [video](https://pan.baidu.com/s/1YxRK7dM8IchfXSMCqoi9JA?pwd=6shh) <br> [summary](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/Paper%20files/李文锋-co-attention机制在可解释虚假新闻与网络欺凌检测的应用(1).doc) |
| 3.29      | 潘金伟    | LIME | [KDD16] ["Why Should I Trust You?": Explaining the Predictions of Any Classifier.](https://arxiv.org/abs/1602.04938) <br> [AAAI18] [Anchors: High-Precision Model-Agnostic Explanations.](http://sameersingh.org/files/papers/anchors-aaai18.pdf) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0329-潘金伟-LIME.pptx)  <br> [video](https://pan.baidu.com/s/1fgTDf87LguPHS3ku2Zieew?pwd=w1il) <br> [summary](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/Paper%20files/潘金伟-基于扰动策略的可解释性方法.doc) |
| 3.29      | 步一凡    | 黑盒模型的解释方法 | [TVCG19] [DeepVID: Deep Visual Interpretation and Diagnosis for Image Classifiers via Knowledge Distillation](https://ieeexplore.ieee.org/document/8667661/) <br> [AIES19] [Faithful and Customizable Explanations of Black Box Models](https://cs.stanford.edu/people/jure/pubs/explanations-aies19.pdf) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0329-步一凡-黑盒模型的解释方法.pptx)  <br> [video](https://pan.baidu.com/s/1fgTDf87LguPHS3ku2Zieew?pwd=w1il) <br> [summary](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/Paper%20files/步一凡-基于黑盒模型的可解释方法.docx) |
| 4.12      | 刘文炎 | IF | [ICML17] [Understanding Black-box Predictions via Influence Functions.](https://arxiv.org/pdf/1703.04730.pdf) <br> [ICLR21] [Influence Functions in Deep Learning Are Fragile.](https://arxiv.org/pdf/2006.14651.pdf) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0412-刘文炎-Influence%20Functions.pdf)  <br> [video](https://pan.baidu.com/s/1-PflanxP2gtpUyh4TYSnBA?pwd=iava) <br> [summary](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/Paper%20files/刘文炎%2B通过影响力函数理解黑盒预测.docx) |
| 4.12      | 郑海坤 | IF | [NeurIPS 18] [Representer Point Selection for Explaining Deep Neural Networks.](https://arxiv.org/abs/1811.09720) <br> [NeurIPS 20] [Estimating Training Data Influence by Tracing Gradient Descent.](https://arxiv.org/abs/2002.08484) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0412-郑海坤-Representer%20Value%20%26%20TracIn.pptx)  <br> [video](https://pan.baidu.com/s/1-PflanxP2gtpUyh4TYSnBA?pwd=iava) <br> [summary](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/Paper%20files/郑海坤-Representer%20Value%20%26%20TracIn.docx) |
| 4.19       | 朱威    | Knowledge Neuron | [ACL22] [Knowledge Neurons in Pretrained Transformers](https://aclanthology.org/2022.acl-long.581/) <br> [Kformer: Knowledge Injection in Transformer Feed-Forward Layers](https://arxiv.org/abs/2201.05742) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0419-朱威-Knowledge%20Neuron.pdf)  <br> [video]() <br> [summary]() |
| 4.19       | 岑黎彬 | Causal+NLP2  | [AAAI21] [Sketch and Customize: A Counterfactual Story Generator.](https://arxiv.org/abs/2104.00929) <br> [NeurlPS21] [Using Embeddings to Estimate Peer Influence on Social Networks.](https://why21.causalai.net/papers/WHY21_41.pdf)  | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0419-岑黎彬-Causal%2BNLP2.pptx)  <br> [video]() <br> [summary](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/Paper%20files/岑黎彬-反事实在NLP的应用以及因果推断在社交网络分析的作用.doc) |
| 4.26      | 江宇辉    | SHAP | [NeurIPS17] [A Unified Approach to Interpreting Model Predictions.](https://arxiv.org/abs/1705.07874) <br> [ICML19] [Data Shapley: Equitable Valuation of Data for Machine Learning.](http://proceedings.mlr.press/v97/ghorbani19c.html) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0426-江宇辉-SHAP.pptx)  <br> [video](https://pan.baidu.com/s/1fw-c__chtg8uufsF3mVdXw?pwd=javc) <br> [summary](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/Paper%20files/江宇辉-SHARP.doc) |
| 4.26      | 孔维璟 | Estimating | [AAAI20] [Learning Counterfactual Representations for Estimating Individual Dose-Response Curves.](https://arxiv.org/abs/1902.00981) <br> [NeurIPS20] [Estimating the Effects of Continuous-valued Interventions using Generative Adversarial Networks.](https://arxiv.org/abs/2002.12326)| [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0426-孔维璟-Estimating.pptx)  <br> [video](https://pan.baidu.com/s/1fw-c__chtg8uufsF3mVdXw?pwd=javc) <br> [summary](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/Paper%20files/孔维璟-反事实.docx) |
| 5.3      |  端午假期 |
| 5.10      | 毛炜    |  Evaluating(NLP) | [ACL20] [Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?](https://arxiv.org/pdf/2005.01831) <br> [NAACL19] [Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications..](https://arxiv.org/pdf/1905.00563.pdf) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0510-毛炜-Evaluating(NLP).pptx)  <br> [video](https://pan.baidu.com/s/1-PflanxP2gtpUyh4TYSnBA?pwd=iava) <br> [summary](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/Paper%20files/毛炜_可解释性在NLP中的评估.doc) |
| 5.10      | 王鹏飞 | Saliency Map | [ICML17] [Axiomatic Attribution for Deep Networks.](https://arxiv.org/abs/1703.01365) <br> [ICML17] [Learning Important Features Through Propagating Activation Differences.](https://arxiv.org/pdf/1704.02685.pdf) | [ppt](https://pan.baidu.com/s/1-PflanxP2gtpUyh4TYSnBA?pwd=iava)  <br> [video]() <br> [summary](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/Paper%20files/王鹏飞_多模态模型可解释性.docx) |
| 5.17      | 岳文静    | GNN Exp.+推荐系统 | [KDD21] [MixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems.](https://dl.acm.org/doi/pdf/10.1145/) <br> [SIGIR21] [Structured Graph Convolutional Networks with Stochastic Masks for Recommender Systems.](https://dl.acm.org/doi/10.1145/3404835.3462868)  | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0517-岳文静-GNN.exp%2Brec.pdf)  <br> [video](https://pan.baidu.com/s/11ydX1O1ftwZTydaFAaxVrw?pwd=o00a) <br> [summary](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/Paper%20files/岳文静-GNN可解释性在推荐中的应用.doc) |
| 5.17      | 钟博 | GNN Exp.1 | [NeurIPS19] [Gnnexplainer: Generating explanations for graph neural networks.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7138248/) <br> [NeurIPS20] [PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks.](https://arxiv.org/pdf/2010.05788.pdf) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0517-钟博-%20时间序列可解释性.pdf)  <br> [video](https://pan.baidu.com/s/11ydX1O1ftwZTydaFAaxVrw?pwd=o00a) <br> [summary](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/Paper%20files/钟博-时间序列可解释性.doc) |
| 5.24      | 郑焕然    | Causal+NLP1  | [ACL21] [Counterfactual Data Augmentation for Neural Machine Translation.](https://www.aclweb.org/anthology/2021.naacl-main.18/) <br> [NAACL21] [Everything Has a Cause: Leveraging Causal Inference in Legal Text Analysis.](https://arxiv.org/abs/2104.09420) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0524-郑焕然-因果推理.pdf)  <br> [video](https://pan.baidu.com/s/1brfXUVEEv5Jk2ITeK0OOLw?pwd=wa33) <br> [summary](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/Paper%20files/郑焕然_因果推理的下游应用.doc) |
| 5.24      | 李靖东 |  GNN Exp. Subgraph | [ICLR21] [Graph Information Bottleneck for Subgraph Recognition.](https://arxiv.org/pdf/2010.05563.pdf) <br> [TPAMI21] [GNN-SubNet: disease subnetwork detection with explainable Graph Neural Networks.](https://www.biorxiv.org/content/10.1101/2022.01.12.475995v1) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0524-李靖东-GNN.subgraph.pptx)  <br> [video](https://pan.baidu.com/s/1brfXUVEEv5Jk2ITeK0OOLw?pwd=wa33) <br> [summary]() |
| 5.31      | 黄佳胤 | GNN Exp.+推荐系统 | [CIKM21] [UltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation](https://arxiv.org/pdf/2110.15114.pdf) <br>  [WSDM22] [Contrastive Meta Learning with Behavior Multiplicity for Recommendation](https://arxiv.org/pdf/2202.08523.pdf) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0530-黄佳胤-GNN%2B推荐系统.pptx)  <br> [video](https://pan.baidu.com/s/1OLo-bGJryWkRRNReQMmaLg?pwd=j3n0) <br> [summary](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/Paper%20files/黄佳胤--讨论班论文整理.doc) |
| 5.31      | 王鹏飞 |  |  | [ppt]()  <br> [video](https://pan.baidu.com/s/1OLo-bGJryWkRRNReQMmaLg?pwd=j3n0) <br> [summary]() |
| 6.7      | 张启凡 | Molecular Generation |  [JCIM22] [MolGPT: Molecular Generation Using a Transformer-Decoder Model](https://pubs.acs.org/doi/10.1021/acs.jcim.1c00600)  <br> [ICLR22] [Data-Efficient Graph Grammar Learning for Molecular Generation](https://arxiv.org/abs/2203.08031) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0607-张启凡-Explainability%20on%20Molecular%20Generationf.pdf)  <br> [video](https://pan.baidu.com/s/152Zmw0XxDzvKvMwsaohSOA?pwd=l86j) <br> [summary](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/Paper%20files/张启凡-GNN可解释性在分子生成中的应用.doc) |
| 6.7      | 屈稳稳    | [Counterfactual Explanations | Counterfactual Explanations for Machine Learning: A Review](https://arxiv.org/abs/2010.10596) <br> [NeurIPS19] [Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers](https://arxiv.org/abs/1912.03277) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0607-屈稳稳-Counterfactual%20Explanations.pptx)  <br> [video](https://pan.baidu.com/s/152Zmw0XxDzvKvMwsaohSOA?pwd=l86j) <br> [summary]() |
| 6.15      | 刘文炎 | DP+XAI | [Model Explanations with Differential Privacy](https://arxiv.org/abs/2006.09129) <br> [CVPR21] [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0614-刘文炎-DP%2BXAI.pptx)   |
| 6.15      | 纪焘 | Attention Flow+Probing Task | [ACL20] [Quantifying Attention Flow in Transformers](https://aclanthology.org/2020.acl-main.385/) <br> [ACL22] [Probing for Labeled Dependency Trees](https://aclanthology.org/2022.acl-long.532.pdf) | [ppt](https://github.com/ECNU109/Explanation-CausalAnalysis-Seminar/blob/main/slides/0614-纪焘-AttnFlow%26Probing.pdf)  |

## 论文主题列表
| 序号 | 主题  | 论文  |
| ---- | ----  | ----  |
|  | PART 1  |  |
|1 | ~~Intro.~~ | ~~[AAAI21 Tutorial] [Explaining Machine Learning Predictions: State-of-the-art, Challenges, and Opportunities](https://explainml-tutorial.github.io/aaai21)~~ |
|2 |  LIME | [KDD16] ["Why Should I Trust You?": Explaining the Predictions of Any Classifier.](https://arxiv.org/abs/1602.04938) <br> [AAAI18] [Anchors: High-Precision Model-Agnostic Explanations.](http://sameersingh.org/files/papers/anchors-aaai18.pdf) |
|3 |  SHAP | [NeurIPS17] [A Unified Approach to Interpreting Model Predictions.](https://arxiv.org/abs/1705.07874) <br> [ICML19] [Data Shapley: Equitable Valuation of Data for Machine Learning.](http://proceedings.mlr.press/v97/ghorbani19c.html) |
|4 | Saliency Map | [ICML17] [Axiomatic Attribution for Deep Networks.](https://arxiv.org/abs/1703.01365) <br> [ICML17] [Learning Important Features Through Propagating Activation Differences.](https://arxiv.org/pdf/1704.02685.pdf) |
|5 |  IF | [ICML17] [Understanding Black-box Predictions via Influence Functions.](https://arxiv.org/pdf/1703.04730.pdf) <br> [ICLR21] [Influence Functions in Deep Learning Are Fragile.](https://arxiv.org/pdf/2006.14651.pdf) |
|6 | Counterfactual |  [Counterfactual Explanations for Machine Learning: A Review.](https://arxiv.org/abs/2010.10596) <br> [NeurIPS19] [Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers.](https://arxiv.org/pdf/1912.03277.pdf) |
|7 | Distill+TCAV | [AIES19] [Faithful and Customizable Explanations of Black Box Models.](https://cs.stanford.edu/people/jure/pubs/explanations-aies19.pdf) <br> [ICML18] [Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV).](https://arxiv.org/pdf/1711.11279.pdf) |
|8 | Evaluating(CV) | [FAT19] [On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection.](https://arxiv.org/pdf/1811.07901.pdf) <br> [AAAI20] [Visualizing Deep Networks by Optimizing with Integrated Gradients.](https://arxiv.org/pdf/1905.00954.pdf) |
|9 | Evaluating(NLP) | [ACL20] [Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?](https://arxiv.org/pdf/2005.01831) <br> [NAACL19] [Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications..](https://arxiv.org/pdf/1905.00563.pdf) |
| | PART 2  |  |
|10 | GNN Exp.1 | [NeurIPS19] [Gnnexplainer: Generating explanations for graph neural networks.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7138248/) <br> [NeurIPS20] [PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks.](https://arxiv.org/pdf/2010.05788.pdf) |
|11 | GNN Exp.2 | [KDD20] [Xgnn: Towards model-level explanations of graph neural networks.](https://dl.acm.org/doi/pdf/10.1145/3394486.3403085) <br> [KDD21] [When Comparing to Ground Truth is Wrong: On Evaluating GNN Explanation Methods.](https://dl.acm.org/doi/10.1145/3447548.3467283) |
| 12| GNN Exp. Pro | [ICML21] [Generative Causal Explanations for Graph Neural Networks.](https://arxiv.org/pdf/2104.06643.pdf) <br> [TPAMI21] [Higher-order explanations of graph neural networks via relevant walks.](https://arxiv.org/pdf/2006.03589.pdf) |
| 13| GNN Exp. Subgraph | [ICLR21] [Graph Information Bottleneck for Subgraph Recognition.](https://arxiv.org/pdf/2010.05563.pdf) <br> [TPAMI21] [GNN-SubNet: disease subnetwork detection with explainable Graph Neural Networks.](https://www.biorxiv.org/content/10.1101/2022.01.12.475995v1) |
|14 | GNN Exp. NLP | [ICLR22] [Discovering Invariant Rationales for Graph Neural Networks.](https://arxiv.org/abs/2201.12872) <br> [ICLR21] [Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking.](https://arxiv.org/abs/2010.00577) |
|15 | GNN Exp.+计算病理 | [CVPR21] [Quantifying Explainers of Graph Neural Networks in Computational Pathology.](https://arxiv.org/pdf/2011.12646.pdf) <br> [ICML21] [Improving Molecular Graph Neural Network Explainability with Orthonormalization and Induced Sparsity.](https://arxiv.org/abs/2105.04854) |
|16 | GNN Exp.+社交网络 | [ACL20] [GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media.](https://arxiv.org/pdf/2004.11648.pdf) <br> [EMNLP20] [HENIN: Learning Heterogeneous Neural Interaction Networks for Explainable Cyberbullying Detection on Social Media.](https://www.aclweb.org/anthology/2020.emnlp-main.200/) |
|17 | GNN Exp.+推荐系统 | [KDD21] [MixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems.](https://dl.acm.org/doi/pdf/10.1145/) <br> [SIGIR21] [Structured Graph Convolutional Networks with Stochastic Masks for Recommender Systems.](https://dl.acm.org/doi/10.1145/3404835.3462868) |
| | PART 3  |   |
|18 | Intro. | [PNAS19] [Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning.](https://arxiv.org/abs/1706.03461) <br> [NeurIPS19] [Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments.](https://arxiv.org/abs/1905.10176) |
|19 | Estimating | [AAAI20] [Learning Counterfactual Representations for Estimating Individual Dose-Response Curves.](https://arxiv.org/abs/1902.00981) <br> [NeurIPS20] [Estimating the Effects of Continuous-valued Interventions using Generative Adversarial Networks.](https://arxiv.org/abs/2002.12326) |
|20 | Temporal data | [ICML20] [Time Series Deconfounder: Estimating Treatment Effects over Time in the Presence of Hidden Confounders.](https://arxiv.org/abs/1902.00450) <br> [ICLR20] [Estimating Counterfactual Treatment Outcomes over Time through Adversarially Balanced Representations.](https://arxiv.org/abs/2002.12326) |
|21 | Structural Causal Models | [NeurIPS20] [Deep Structural Causal Models for Tractable Counterfactual Inference.](https://arxiv.org/abs/2006.06485) <br> [NeurIPS19] [Representation Learning for Treatment Effect Estimation from Observational Data.](https://papers.nips.cc/paper/7529-representation-learning-for-treatment-effect-estimation-from-observational-data.pdf) |
|22 |  Causal+NLP1  | [ACL21] [Counterfactual Data Augmentation for Neural Machine Translation.](https://www.aclweb.org/anthology/2021.naacl-main.18/) <br> [NAACL21] [Everything Has a Cause: Leveraging Causal Inference in Legal Text Analysis.](https://arxiv.org/abs/2104.09420) |
|23 |  Causal+NLP2  | [AAAI21] [Sketch and Customize: A Counterfactual Story Generator.](https://arxiv.org/abs/2104.00929) <br> [NeurlPS21] [Using Embeddings to Estimate Peer Influence on Social Networks.](https://why21.causalai.net/papers/WHY21_41.pdf) |


---

## 阅读资源列表

> ⭐代表推荐阅读

## I. Explanation Technique

⭐[AAAI21 Tutorial] **Explaining Machine Learning Predictions: State-of-the-art, Challenges, and Opportunities** [Talk](https://explainml-tutorial.github.io/aaai21)

⭐[CVPR21 Tutorial] **Interpretable Machine Learning for Computer Vision** [Talk](https://interpretablevision.github.io)

Slides [Part1](https://interpretablevision.github.io/slide/cvpr21_samek.pdf) [Part2](https://interpretablevision.github.io/slide/cvpr21_rudin.pdf) [Part3](https://interpretablevision.github.io/slide/cvpr21_morcos.pdf) [Part4](https://interpretablevision.github.io/slide/cvpr21_zhou.pdf)

### 方法介绍（LIME、SHAP、Saliency Map、IF、Counterfactual）

1. ⭐**Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.** *Cynthia Rudin.* Nature Machine Intelligence 2019. [paper](https://arxiv.org/pdf/1811.10154.pdf)
2. ⭐**The Mythos of Model Interpretability.** *Zachary C. Lipton.* Machine Learning 2018. [paper](https://arxiv.org/abs/1606.03490)
3. ⭐[LIME] **"Why Should I Trust You?": Explaining the Predictions of Any Classifier.** *MT Ribeiro, S Singh, C Guestrin.* KDD 2016. [paper](https://arxiv.org/abs/1602.04938)
4. ⭐[Anchors] **Anchors: High-Precision Model-Agnostic Explanations.** *MT Ribeiro, S Singh, C Guestrin.* AAAI 2018. [paper](http://sameersingh.org/files/papers/anchors-aaai18.pdf)
5. ⭐[SHAP] **A Unified Approach to Interpreting Model Predictions.** *Scott Lundberg, Su-In Lee.* NeurIPS 2017. [paper](https://arxiv.org/abs/1705.07874)
6. ⭐[SHAP] **Data Shapley: Equitable Valuation of Data for Machine Learning.** *Amirata Ghorbani, James Zou.* ICML 2019. [paper](http://proceedings.mlr.press/v97/ghorbani19c.html)
7. ⭐[Saliency Map] **Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps.** *Karen Simonyan, Andrea Vedaldi, Andrew Zisserman.* ICLR ws 2014. [paper](https://arxiv.org/abs/1312.6034)
8. ⭐[Saliency Map-DeepLIFT] **Learning Important Features Through Propagating Activation Differences.** *Avanti Shrikumar, Peyton Greenside, Anshul Kundaje*. ICML 2017. [paper](https://arxiv.org/pdf/1704.02685.pdf)
9. ⭐[SmoothGrad] **Noise-adding Methods of Saliency Map as Series of Higher Order Partial Derivative.** *Junghoon Seo, Jeongyeol Choe, Jamyoung Koo, Seunghyeon Jeon, Beomsu Kim, Taegyun Jeon.* ICML ws 2018. [paper](https://arxiv.org/pdf/1806.03000.pdf)
10. ⭐[Saliency Map-积分梯度] **Axiomatic Attribution for Deep Networks.** *Mukund Sundararajan, Ankur Taly, Qiqi Yan*. ICML 2017. [paper](https://arxiv.org/abs/1703.01365)
11. ⭐[Saliency Map-RISE] **RISE: randomized input sampling for explanation of black-box models.** BMVC 2018. [paper](https://arxiv.org/abs/1806.07421)
12. ⭐[IF] **Understanding Black-box Predictions via Influence Functions.** ICML 2017. [paper](https://arxiv.org/pdf/1703.04730.pdf)
13. [IF] **Influence Functions in Deep Learning Are Fragile.** ICLR 2021. [paper](https://arxiv.org/pdf/2006.14651.pdf)
14. [IF] **Representer Point Selection for Explaining Deep Neural Networks.** NeurIPS 2018. [paper](https://arxiv.org/abs/1811.09720)
15. [IF] **Estimating Training Data Influence by Tracing Gradient Descent.** NeurIPS 2020. [paper](https://arxiv.org/abs/2002.08484)
16. ⭐[Counterfactual] **Counterfactual Explanations for Machine Learning: A Review.** [paper](https://arxiv.org/abs/2010.10596)
17. [Counterfactual] **Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR**. [paper](https://arxiv.org/abs/1711.00399)
18. [Counterfactual] **Model-agnostic counterfactual explanations for consequential decisions**. *AISTATS 2020.* [paper](https://arxiv.org/abs/1905.11190)
19. [Counterfactual] **Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers.** NeurIPS 2019. [paper](https://arxiv.org/pdf/1912.03277.pdf)
20. [Distill] **Faithful and Customizable Explanations of Black Box Models.** AIES 2019. [paper](https://cs.stanford.edu/people/jure/pubs/explanations-aies19.pdf)
21. ⭐[Distill] **“How do I fool you?": Manipulating User Trust via Misleading Black Box Explanations.** AIES 2020. [paper](https://www.aies-conference.com/2020/wp-content/papers/182.pdf)
22. [Distill] **EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.** ICML 2019. [paper](https://arxiv.org/pdf/1905.11946.pdf)
23. ⭐[Counterfactual] **Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses.** NeurIPS 2020. [paper](https://arxiv.org/pdf/2009.07165)
24. [Dissection] **Network Dissection: Quantifying Interpretability of Deep Visual Representations.** CVPR 2017. [paper](http://netdissect.csail.mit.edu)
25. ⭐[TCAV] **Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV).** ICML 2018. [paper](https://arxiv.org/pdf/1711.11279.pdf)
26. [TCAV] **Regression Concept Vectors for Bidirectional Explanations in Histopathology.** MICCAI 2018. [paper](https://arxiv.org/abs/1904.04520)
27. [TCAV] **Towards Automatic Concept-based Explanations.** NeurIPS 2019. [paper](https://arxiv.org/pdf/1902.03129.pdf)

### 解释性方法评测

1. ⭐**On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection.** FAT 2019. [paper](https://arxiv.org/pdf/1811.07901.pdf)
2. ⭐**Teaching Categories to Human Learners with Visual Explanations.** CVPR 2018. [paper](https://arxiv.org/pdf/1802.06924.pdf)
3. ⭐**Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?.** ACL 2020. [paper](https://arxiv.org/pdf/2005.01831)
4. **Visualizing Deep Networks by Optimizing with Integrated Gradients.** AAAI 2020. [paper](https://arxiv.org/pdf/1905.00954.pdf)
5. **Towards A Rigorous Science of Interpretable Machine Learning.** arxiv 2017. [paper](https://arxiv.org/abs/1702.08608)
6. **Manipulating and Measuring Model Interpretability.** CHI 2021. [paper](https://arxiv.org/pdf/1902.03129.pdf)
7. **Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications.** NAACL 2019. [paper](https://arxiv.org/pdf/1905.00563.pdf)


## II. Graph Explainability

### 网络结构解释
1. ⭐**Gnnexplainer: Generating explanations for graph neural networks**. *Ying Rex, Bourgeois Dylan, You Jiaxuan, Zitnik Marinka, Leskovec Jure*. NeurIPS 2019. [paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7138248/) [code](https://github.com/RexYing/gnn-model-explainer)
2. ⭐**PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks**. *Vu Minh, Thai My T.*. NeurIPS 2020. [paper](https://arxiv.org/pdf/2010.05788.pdf)
3. ⭐**Xgnn: Towards model-level explanations of graph neural networks**. *Yuan Hao, Tang Jiliang, Hu Xia, Ji Shuiwang*. KDD 2020. [paper](https://dl.acm.org/doi/pdf/10.1145/3394486.3403085)
4. ⭐**When Comparing to Ground Truth is Wrong: On Evaluating GNN Explanation Methods**. *Faber Lukas, K. Moghaddam Amin, Wattenhofer Roger*. KDD 2021. [paper](https://dl.acm.org/doi/10.1145/3447548.3467283)
5. ⭐**Generative Causal Explanations for Graph Neural Networks**. *Lin Wanyu, Lan Hao, Li Baochun*. ICML 2021. [paper](https://arxiv.org/pdf/2104.06643.pdf)
6. ⭐**Higher-order explanations of graph neural networks via relevant walks**. *Schnake Thomas, Eberle Oliver, Lederer Jonas, Nakajima Shinichi, Schütt Kristof T, Müller Klaus-Robert, Montavon Grégoire*. TPAMI 2021. [paper](https://arxiv.org/pdf/2006.03589.pdf)
7. ⭐**Discovering Invariant Rationales for Graph Neural Networks.** ICLR 2022. [paper](https://arxiv.org/abs/2201.12872)
8. ⭐**Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking.** ICLR 2021. [paper](https://arxiv.org/abs/2010.00577)
9. ⭐**Multi-objective Explanations of GNN Predictions.** ICDM 2021. [paper](https://arxiv.org/abs/2111.14651)
10. ⭐**Graph Information Bottleneck for Subgraph Recognition.** ICLR 2021. [paper](https://arxiv.org/pdf/2010.05563.pdf)
11. **ProtGNN: Towards Self-Explaining Graph Neural Networks.** AAAI 2022. [paper](https://arxiv.org/abs/2112.00911)
12. **Deconfounding to Explanation Evaluation in Graph Neural Networks.** ICLR 2022. [paper](https://arxiv.org/abs/2201.08802)
13. **GNN-SubNet: disease subnetwork detection with explainable Graph Neural Networks.** BioRxiv 2022. [paper](https://www.biorxiv.org/content/10.1101/2022.01.12.475995v1)
14. **Learning and Evaluating Graph Neural Network Explanations based on Counterfactual and Factual Reasoning.** The Webconf 22. [paper](https://arxiv.org/abs/2202.08816)
15. **EGNN: Constructing explainable graph neural networks via knowledge distillation. KBS 2022.** [paper](https://www.sciencedirect.com/science/article/pii/S0950705122001289?via%3Dihub)
16. **Reinforcement Learning Enhanced Explainer for Graph Neural Networks.** NeurIPS 2021. [paper](http://recmind.cn/papers/explainer_nips21.pdf)
17. **Towards Multi-Grained Explainability for Graph Neural Networks.** NeurIPS 2021. [paper](http://staff.ustc.edu.cn/~hexn/papers/nips21-explain-gnn.pdf)
18. **Robust Counterfactual Explanations on Graph Neural Networks.** NeurIPS 2021. [paper](https://arxiv.org/abs/2107.04086)
19. **On Explainability of Graph Neural Networks via Subgraph Explorations.** ICML 2021. [paper](https://arxiv.org/abs/2102.05152)
20. **Automated Graph Representation Learning with Hyperparameter Importance Explanation.** ICML 2021. [paper](http://proceedings.mlr.press/v139/wang21f/wang21f.pdf)


### 图解释性的下游应用
### [1-4]计算病理  [5-7]社交网络  [8-10]推荐系统

1. ⭐**Quantifying Explainers of Graph Neural Networks in Computational Pathology**. *Jaume Guillaume, Pati Pushpak, Bozorgtabar Behzad, Foncubierta Antonio, Anniciello Anna Maria, Feroce Florinda, Rau Tilman, Thiran Jean-Philippe, Gabrani Maria, Goksel Orcun*. CVPR 2021. [paper](https://arxiv.org/pdf/2011.12646.pdf)
2. ⭐**Counterfactual Supporting Facts Extraction for Explainable Medical Record Based Diagnosis with Graph Network**. *Wu Haoran, Chen Wei, Xu Shuang, Xu Bo*. NAACL 2021. [paper](https://aclanthology.org/2021.naacl-main.156.pdf)
3. ⭐**Counterfactual Graphs for Explainable Classification of Brain Networks**. *Abrate Carlo, Bonchi Francesco*. KDD 2021. [paper](https://arxiv.org/pdf/2106.08640.pdf)
4. **Improving Molecular Graph Neural Network Explainability with Orthonormalization and Induced Sparsity.** ICML 2021**.** [paper](https://arxiv.org/abs/2105.04854)
5. ⭐**GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media**. *Lu, Yi-Ju and Li, Cheng-Te*. ACL 2020. [paper](https://arxiv.org/pdf/2004.11648.pdf)
6. ⭐**HENIN: Learning Heterogeneous Neural Interaction Networks for Explainable Cyberbullying Detection on Social Media**. *Chen, Hsin-Yu and Li, Cheng-Te*. EMNLP 2020. [paper](https://www.aclweb.org/anthology/2020.emnlp-main.200/)
7. ⭐**SCARLET: Explainable Attention based Graph Neural Network for Fake News spreader prediction.** PAKDD 21 **.** [paper](https://arxiv.org/abs/2102.04627)
8. ⭐**MixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems.** *Tinglin Huang, Yuxiao Dong, Ming Ding, Zhen Yang, Wenzheng Feng, Xinyu Wang, Jie Tang*. KDD 2021. [paper](https://dl.acm.org/doi/pdf/10.1145/3447548.3467408) [code](https://github.com/huangtinglin/MixGCF)
9. ⭐**Structured Graph Convolutional Networks with Stochastic Masks for Recommender Systems.** *Chen, Huiyuan and Wang, Lan and Lin, Yusan and Yeh, Chin-Chia Michael and Wang, Fei and Yang, Hao*. SIGIR 2021. [paper](https://dl.acm.org/doi/10.1145/3404835.3462868) [code](https://github.com/THUDM/cogdl/blob/master)
10. ⭐**Sequential Recommendation with Graph Convolutional Networks.** *Jianxin Chang, Chen Gao, Yu Zheng, Yiqun Hui, Yanan Niu, Yang Song, Depeng Jin, Yong Li*. SIGIR 2021. [paper](https://arxiv.org/abs/2106.14226) [code](https://github.com/THUDM/cogdl/blob/master)

## III.  Causal Inference

1. ⭐[**Survey] Toward Causal Representation Learning**, IEEE 2021. [paper](https://ieeexplore.ieee.org/abstract/document/9363924)
2. ⭐[**Survey] A Survey of Learning Causality with Data: Problems and Methods**, ACM 2020. [paper](https://arxiv.org/abs/1809.09337)

### 因果推断方法介绍

1. ⭐**Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning.** PNAS 2019. [paper](https://arxiv.org/abs/1706.03461)
2. ⭐**Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments.** NeurIPS 2019. [paper](https://arxiv.org/abs/1905.10176)
3. ⭐**VCNet and Functional Targeted Regularization For Learning Causal Effects of Continuous Treatments.** ICLR 2021. [paper](https://arxiv.org/abs/2103.07861)  [code](https://github.com/lushleaf/varying-coefficient-net-with-functional-tr)
4. ⭐**Learning Counterfactual Representations for Estimating Individual Dose-Response Curves.** AAAI 2020. [paper](https://arxiv.org/abs/1902.00981) [code](https://github.com/d909b/drnet)
5. ⭐**Estimating the Effects of Continuous-valued Interventions using Generative Adversarial Networks.** NeurIPS 2020. [paper](https://arxiv.org/abs/2002.12326) [code](https://github.com/ioanabica/SCIGAN)
6. **Learning Individual Causal Effects from Networked Observational Data.** WSDM 2020. [paper](https://arxiv.org/abs/1906.03485) [code](https://github.com/rguo12/network-deconfounder-wsdm20)
7. ⭐[Temporal data] **Time Series Deconfounder: Estimating Treatment Effects over Time in the Presence of Hidden Confounders.** ICML 2020. [paper](https://arxiv.org/abs/1902.00450) [code](https://github.com/ioanabica/Time-Series-Deconfounder)
8. ⭐[Temporal data] **Estimating Counterfactual Treatment Outcomes over Time through Adversarially Balanced Representations.** ICLR 2020. [paper](https://openreview.net/pdf?id=BJg866NFvB) [code](https://github.com/ioanabica/Counterfactual-Recurrent-Network)
9. ⭐**Deep Structural Causal Models for Tractable Counterfactual Inference.** NeurIPS 2020. [paper](https://arxiv.org/abs/2006.06485) [code](https://github.com/biomedia-mira/deepscm)
10. **Representation Learning for Treatment Effect Estimation from Observational Data.** NeurIPS 2019. [paper](https://papers.nips.cc/paper/7529-representation-learning-for-treatment-effect-estimation-from-observational-data.pdf)
11. **Differentiable Causal Discovery Under Unmeasured Confounding.** AISTATS 2021. [paper](https://arxiv.org/abs/2010.06978)

### 因果推断的下游应用

1. ⭐**Counterfactual Data Augmentation for Neural Machine Translation.** ACL 2021. [paper](https://www.aclweb.org/anthology/2021.naacl-main.18/) [code](https://github.com/xxxiaol/GCI)
2. ⭐**Everything Has a Cause: Leveraging Causal Inference in Legal Text Analysis.** NAACL 2021. [paper](https://arxiv.org/abs/2104.09420) [code](https://github.com/xxxiaol/GCI)
3. ⭐**Causal Effects of Linguistic Properties.** NAACL, 2021. [paper](https://arxiv.org/abs/2010.12919)
4. **Sketch and Customize: A Counterfactual Story Generator.** AAAI, 2021. [paper](https://arxiv.org/abs/2104.00929)
5. **Counterfactual Generator: A Weakly-Supervised Method for Named Entity Recognition.** EMNLP 2020. [paper](https://github.com/xijiz/cfgen/blob/master/docs/cfgen.pdf) [code](https://github.com/xijiz/cfgen)
6. ⭐**The Deconfounded Recommender: A Causal Inference Approach to Recommendation.** *arXiv*, 2019. [paper](https://arxiv.org/abs/1808.06581)  [code](https://github.com/blei-lab/deconfounder_tutorial)
7. **Using Embeddings to Estimate Peer Influence on Social Networks. NeurlPS 2021. [paper](https://why21.causalai.net/papers/WHY21_41.pdf)**
8. **Unsupervised Causal Binary Concepts Discovery with VAE for Black-box Model Explanation. NeurlPS 2021. [paper](https://why21.causalai.net/papers/WHY21_3.pdf)**
9. ⭐**Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models.** ACL 2021. [paper](https://arxiv.org/abs/2106.06087)
