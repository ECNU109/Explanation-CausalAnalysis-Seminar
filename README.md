# Explanation-CausalAnalysis-Seminar
2022Spring Seminar: 1. Explanation Technique 2. Graph Explainability 3. Causal Analysis

时间：周二 9:00 am

地点：TBD

---

## 论文报告安排

|  序号   | 报告人  | 主题  | 论文  |
|  ----  | ---- | ----  | ----  |
| 1      | 纪焘 | Intro. | [AAAI21 Tutorial] [Explaining Machine Learning Predictions: State-of-the-art, Challenges, and Opportunities](https://explainml-tutorial.github.io/aaai21) |
| 2      |  |  LIME | [KDD16] ["Why Should I Trust You?": Explaining the Predictions of Any Classifier.](https://arxiv.org/abs/1602.04938) <br> [AAAI18] [Anchors: High-Precision Model-Agnostic Explanations.](http://sameersingh.org/files/papers/anchors-aaai18.pdf) |
| 2      |  |  SHAP | [NeurIPS17] [A Unified Approach to Interpreting Model Predictions.](https://arxiv.org/abs/1705.07874) <br> [ICML19] [Data Shapley: Equitable Valuation of Data for Machine Learning.](http://proceedings.mlr.press/v97/ghorbani19c.html) |
| 3      |  | Saliency Map | [ICLR ws14] [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps.](https://arxiv.org/abs/1312.6034) <br> [ICML17] [Learning Important Features Through Propagating Activation Differences.](https://arxiv.org/pdf/1704.02685.pdf) |
| 3      |  | Saliency Map | [ICMLws18] [Noise-adding Methods of Saliency Map as Series of Higher Order Partial Derivative.](https://arxiv.org/abs/1705.07874) <br> [ICML17] [Axiomatic Attribution for Deep Networks.](http://proceedings.mlr.press/v97/ghorbani19c.html) |
| 4      |  |  |  |
| 4      |  |  |  |
| 5      |  |  |  |
| 5      |  |  |  |
| 6      |  |  |  |
| 6      |  |  |  |
| 7      |  |  |  |
| 7      |  |  |  |
| 8      |  |  |  |
| 8      |  |  |  |
| 9      |  |  |  |
| 9      |  |  |  |
| 10     |  |  |  |
| 10     |  |  |  |

---

## 阅读资源列表

> ⭐代表推荐阅读

### I. Explanation Technique

⭐[AAAI21 Tutorial] **Explaining Machine Learning Predictions: State-of-the-art, Challenges, and Opportunities** [Talk](https://explainml-tutorial.github.io/aaai21)

⭐[CVPR21 Tutorial] **Interpretable Machine Learning for Computer Vision** [Talk](https://interpretablevision.github.io)

Slides [Part1](https://interpretablevision.github.io/slide/cvpr21_samek.pdf) [Part2](https://interpretablevision.github.io/slide/cvpr21_rudin.pdf) [Part3](https://interpretablevision.github.io/slide/cvpr21_morcos.pdf) [Part4](https://interpretablevision.github.io/slide/cvpr21_zhou.pdf)

### 方法介绍（LIME、SHAP、Saliency Map、IF、Counterfactual）

1. ⭐**Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.** *Cynthia Rudin.* Nature Machine Intelligence 2019. [paper](https://arxiv.org/pdf/1811.10154.pdf)
2. ⭐**The Mythos of Model Interpretability.** *Zachary C. Lipton.* Machine Learning 2018. [paper](https://arxiv.org/abs/1606.03490)
3. ⭐[LIME] **"Why Should I Trust You?": Explaining the Predictions of Any Classifier.** *MT Ribeiro, S Singh, C Guestrin.* KDD 2016. [paper](https://arxiv.org/abs/1602.04938)
4. ⭐[Anchors] **Anchors: High-Precision Model-Agnostic Explanations.** *MT Ribeiro, S Singh, C Guestrin.* AAAI 2018. [paper](http://sameersingh.org/files/papers/anchors-aaai18.pdf)
5. ⭐[SHAP] **A Unified Approach to Interpreting Model Predictions.** *Scott Lundberg, Su-In Lee.* NeurIPS 2017. [paper](https://arxiv.org/abs/1705.07874)
6. ⭐[SHAP] **Data Shapley: Equitable Valuation of Data for Machine Learning.** *Amirata Ghorbani, James Zou.* ICML 2019. [paper](http://proceedings.mlr.press/v97/ghorbani19c.html)
7. ⭐[Saliency Map] **Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps.** *Karen Simonyan, Andrea Vedaldi, Andrew Zisserman.* ICLR ws 2014. [paper](https://arxiv.org/abs/1312.6034)
8. ⭐[Saliency Map-DeepLIFT] **Learning Important Features Through Propagating Activation Differences.** *Avanti Shrikumar, Peyton Greenside, Anshul Kundaje*. ICML 2017. [paper](https://arxiv.org/pdf/1704.02685.pdf)
